{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 # Recommender Systems Implementations: \
* Retailrocket recommender system dataset: I have designed a recommender system based on the "event.csv" file of this dataset which is collected from an ecommerce website. This file contains the behavior data of users of an ecommerce website. We would like to predict the future behavior of users for specific items based on their previous behaviors towards different items. These behaviors include: viewing the item, adding to cart and placing an order.\
* I have used Pyspark (A Python API that supports Spark for the big data analysis) and implemented the recommneder algorithm on *Data Bricks* clusters.\
\
https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/7194191126325765/1471038362177880/6285802778455178/latest.html\
\
* Retailrocket dataset has 2,756,101 events including 2,664,312 views, 69,332 add-to-carts and 22,457 transactions produced by 1,407,580 unique visitors.\
We have just used one of the datasets which is called "events.csv" and it has 5 columns: timestamp, visitor id, event, timeid and transaction id.\
\
* The necessary steps towards solving recommender system problem are as following:\
\
1. Data Wrangling and Pre-processing:  Since we have raw data lots of data wrangling and preprocessing is needed such as removing the unnecessary columns like timestamp, counting the number of the "views", "transactions" and "add-to-carts", casting the datatype to an appropriate one, removing the NaN and NULL values. The dataframes are also changed into RDD (Resilient Distributed Datasets) which is the default for the big data analysis in Spark since it makes the parallel computations faster.\
\
2. Randomly splitting the dataset into training, validation and test (60%,20%,20%).\
\
3. Defining the evaluation function that computes the RMSE for predicted and actual ratings.\
\
4. Finding the best **rank** in the ALS algorithm. The rank shows the number of the latent/hidden factors and it depends on the data. The higher the value of rank the better the predictions BUT it might cause *overfitting* and it will consume more time and memory so finding the best rank is important. \
We can start with ranks 5-10 and then increase it to get a better performance. The best rank is 20 according to the limited computing resources (clusters) on free trail DataBricks account.\
\
5. I have re-defined the *rating* for this problem in order to consider the behaviors of the users. In other words, if the user does not do anything about the item (not interested) the weight/rating is 1, if he just views the item the weight/rating is 2, if he adds the item to his cart the weight is 3 but if he buys the item the weight/rating is set to 5. \
\
}